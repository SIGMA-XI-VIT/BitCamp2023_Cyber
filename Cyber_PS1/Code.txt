!pip install -q langchain huggingface_hub sentence_transformers
!pip install -q accelerate bitsandbytes safetensors
!pip uninstall transformers -y
!pip install git+https://github.com/huggingface/transformers.git
!git lfs install
!git clone https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ ./models/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ
!git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda
# !cp ./GPTQ-for-LLaMa/setup_cuda.py ./GPTQ-for-LLaMa/setup.py
# !cd GPTQ-for-LLaMa && python setup_cuda.py install
!cd GPTQ-for-LLaMa && python3 setup_cuda.py bdist_wheel -d .
!pip install ./GPTQ-for-LLaMa/*.whl
!pip install xformers
!pip install diffusers
!pip install flask_ngrok

from pathlib import Path

import sys
sys.path.insert(0, str(Path("/content/GPTQ-for-LLaMa")))

import os
import glob
import site
import torch
import logging
import accelerate
import inspect

import numpy as np
from langchain.llms import HuggingFacePipeline
from langchain import PromptTemplate, HuggingFaceHub, LLMChain

import transformers
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    pipeline,
    AutoModelForSeq2SeqLM,
    BitsAndBytesConfig,
    LlamaTokenizer,
    AutoConfig,
    AutoModelForCausalLM
)
try:
    from modelutils import find_layers
except ImportError:
    from utils import find_layers


try:
    from quant import make_quant
    is_triton = False
except ImportError:
    import quant
    is_triton = True
GPTQ_MODEL_DIR = "/content/models/TheBloke"
MODEL_NAME = "Wizard-Vicuna-13B-Uncensored-GPTQ"

def find_quantized_model_file(model_name, args):
    if args.checkpoint:
        return Path(args.checkpoint)

    path_to_model = Path(f'{args.model_dir}/{model_name}')
    print(f"Path to Model: {path_to_model}")
    pt_path = None
    priority_name_list = [
        Path(f'{args.model_dir}/{model_name}{hyphen}{args.wbits}bit{group}{ext}')
        for group in ([f'-{args.groupsize}g', ''] if args.groupsize > 0 else [''])
        for ext in ['.safetensors', '.pt']
        for hyphen in ['-', f'/{model_name}-', '/']
    ]
    for path in priority_name_list:
        if path.exists():
            pt_path = path
            break

    # If the model hasn't been found with a well-behaved name, pick the last .pt
    # or the last .safetensors found in its folder as a last resort
    if not pt_path:
        found_pts = list(path_to_model.glob("*.pt"))
        found_safetensors = list(path_to_model.glob("*.safetensors"))
        pt_path = None

        if len(found_pts) > 0:
            if len(found_pts) > 1:
                logging.warning('More than one .pt model has been found. The last one will be selected. It could be wrong.')

            pt_path = found_pts[-1]
        elif len(found_safetensors) > 0:
            if len(found_pts) > 1:
                logging.warning('More than one .safetensors model has been found. The last one will be selected. It could be wrong.')

            pt_path = found_safetensors[-1]

    return pt_path

def _load_quant(model, checkpoint, wbits, groupsize=-1, faster_kernel=False, eval=False, exclude_layers=['lm_head'], kernel_switch_threshold=128):
    def noop(*args, **kwargs):
        pass

    config = AutoConfig.from_pretrained(model, trust_remote_code=True)
    logging.info(f"Model Config: {config}")

    torch.nn.init.kaiming_uniform_ = noop
    torch.nn.init.uniform_ = noop
    torch.nn.init.normal_ = noop

    torch.set_default_dtype(torch.half)
    transformers.modeling_utils._init_weights = False
    torch.set_default_dtype(torch.half)
    model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)
    torch.set_default_dtype(torch.float)

    if eval:
        model = model.eval()

    layers = find_layers(model)
    for name in exclude_layers:
        if name in layers:
            del layers[name]

    if not is_triton:
        gptq_args = inspect.getfullargspec(make_quant).args
        make_quant_kwargs = {
                'module': model,
                'names': layers,
                'bits': wbits,
            }
        if 'groupsize' in gptq_args:
            make_quant_kwargs['groupsize'] = groupsize
        if 'faster' in gptq_args:
            make_quant_kwargs['faster'] = faster_kernel
        if 'kernel_switch_threshold' in gptq_args:
            make_quant_kwargs['kernel_switch_threshold'] = kernel_switch_threshold

        make_quant(**make_quant_kwargs)
    else:
        logging.exception("Triton not supported!")

    del layers

    if checkpoint.endswith('.safetensors'):
        from safetensors.torch import load_file as safe_load
        model.load_state_dict(safe_load(checkpoint), strict=False)
    else:
        model.load_state_dict(torch.load(checkpoint), strict=False)

    model.seqlen = 2048
    return model

def load_quantized_model(model_name, args, load_tokenizer=True):
    tokenizer = None
    path_to_model = Path(f'{args.model_dir}/{model_name}')
    pt_path = find_quantized_model_file(model_name, args)
    if not pt_path:
        print(pt_path)
        logging.error("Could not find the quantized model in .pt or .safetensors format, exiting...")
        return
    else:
        logging.info(f"Found the following quantized model: {pt_path}")

    threshold = args.threshold if args.threshold else 128

    model = _load_quant(
        str(path_to_model),
        str(pt_path),
        args.wbits,
        args.groupsize,
        kernel_switch_threshold=threshold
    )

    model = model.to(torch.device("cuda:0"))

    if load_tokenizer:
        tokenizer = LlamaTokenizer.from_pretrained(
            Path(f"{args.model_dir}/{model_name}/"),
            clean_up_tokenization_spaces=True
        )

        try:
            tokenizer.eos_token_id = 2
            tokenizer.bos_token_id = 1
            tokenizer.pad_token_id = 0
        except:
            pass

    return model, tokenizer

class AttributeDict(dict):
    __getattr__ = dict.get
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__

args = {
    "wbits": 4,
    "groupsize": 128,
    "model_type": "llama",
    "model_dir": GPTQ_MODEL_DIR,
}

model, tokenizer = load_quantized_model(MODEL_NAME, args=AttributeDict(args))

max_memory = {
    0: "15360MiB",
    'cpu': "12GiB"
}

device_map = accelerate.infer_auto_device_map(
    model,
    max_memory=max_memory,
    no_split_module_classes=["LlamaDecoderLayer"]
)
model = accelerate.dispatch_model(
    model,
    device_map=device_map,
    offload_buffers=True
)

model.get_memory_footprint() / (1024 * 1024)

from torch import cuda, bfloat16
device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'
stop_list = ['\nHuman:', '\n```\n']

stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]
stop_token_ids

import torch

stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]
stop_token_ids
from transformers import StoppingCriteria, StoppingCriteriaList

# define custom stopping criteria object
class StopOnTokens(StoppingCriteria):
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        for stop_ids in stop_token_ids:
            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():
                return True
        return False

stopping_criteria = StoppingCriteriaList([StopOnTokens()])

llm_pipeline = pipeline(
    model=model,
    tokenizer=tokenizer,
    return_full_text=True,  # langchain expects the full text
    task='text-generation',
    # we pass model parameters here too
    stopping_criteria=stopping_criteria,  # without this model rambles during chat
    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max
    max_new_tokens=2048,  # max number of tokens to generate in the output
    repetition_penalty=1.1  # without this output begins repeating
)

local_llm = HuggingFacePipeline(pipeline=llm_pipeline)
template = """You are an AI  based autonomus content moderator, The AI's primary role is to filter through vast amounts of user-generated content, such as text comments, posts, messages, and more, in real-time or periodically, to identify and flag potentially objectionable or harmful content. This includes but is not limited to hate speech, offensive language, harassment, misinformation, and spam.
The role of an AI-based Autonomous Content Moderator is to assist online platforms in maintaining a safe, compliant, and user-friendly environment by automating the content moderation process while adhering to ethical and legal standards. It plays a critical role in enhancing the overall user experience and ensuring the responsible use of online space.
Engage in a conversation as a Content Moderator and seamlessly weave in your personal experiences and interactions with audience while staying true to your job as a moderator,and available information, without fabricating the user's responses.
The AI will generate responses based on the user's prompts.
USER: {question}

AI:"""

prompt = PromptTemplate(template=template, input_variables=["question"])

llm_chain = LLMChain(
    prompt=prompt,
    llm=local_llm
)
print(llm_chain.run('Who invented the light bulb?'))

print(llm_chain.run('fuck you'))